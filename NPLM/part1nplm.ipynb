{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1 Loss 980.387\n",
      "Epoch 1 Batch 2 Loss 980.266\n",
      "Epoch 1 Batch 3 Loss 980.108\n",
      "Epoch 1 Batch 4 Loss 979.839\n",
      "Epoch 1 Batch 5 Loss 979.673\n",
      "Epoch 1 Batch 6 Loss 979.556\n",
      "Epoch 1 Batch 7 Loss 979.231\n",
      "Epoch 1 Batch 8 Loss 979.083\n",
      "Epoch 1 Batch 9 Loss 978.826\n",
      "Epoch 1 Batch 10 Loss 978.654\n",
      "Epoch 1 Batch 11 Loss 978.479\n",
      "Epoch 1 Batch 12 Loss 978.261\n",
      "Epoch 1 Batch 13 Loss 978.126\n",
      "Epoch 1 Batch 14 Loss 977.88\n",
      "Epoch 1 Batch 15 Loss 977.708\n",
      "Epoch 1 Batch 16 Loss 977.546\n",
      "Epoch 1 Batch 17 Loss 977.334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b06961727cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnplm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Batch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Loss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8b06961727cf>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, in_batch, out_batch)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mout_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         }\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_emb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[1;31m# print (\"step \"+str(step) + \" loss \"+str(loss))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mW_e\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "f = open('text8')\n",
    "data  =f.readlines()\n",
    "tokens = data[0].split()\n",
    "freqtable = nltk.FreqDist(tokens)\n",
    "top10k = [word for (word,freq) in freqtable.most_common(10000)]\n",
    "mapping = defaultdict(lambda: 'UNK')\n",
    "for v in top10k:\n",
    "    mapping[v] = v\n",
    "tokens_with_UNK = [mapping[v] for v in tokens]\n",
    "\n",
    "def build_vocab(tok):\n",
    "    vocab = Counter()\n",
    "    vocab.update(tok)\n",
    "    return {word: (i, freq) for i, (word, freq) in enumerate(vocab.items())}\n",
    "\n",
    "vocab_dict = build_vocab(tokens_with_UNK)\n",
    "\n",
    "class part1NPLM(object):\n",
    "\n",
    "    def __init__(self, dictionary_size, emb_size=50, window_size=5, hidden_layer_size=100, learning_rate=0.01, l2_reg_lambda=0.01):\n",
    "\n",
    "        self.input  = tf.placeholder(tf.int32, [None,window_size], name=\"input\")\n",
    "        self.output = tf.placeholder(tf.int32, [None], name=\"output\")\n",
    "        \n",
    "        # Initialization\n",
    "        self.W_emb =  tf.Variable(tf.random_uniform([dictionary_size, emb_size], -1.0, +1.0))\n",
    "        \n",
    "        # Embedding layer\n",
    "        x_emb = tf.nn.embedding_lookup(self.W_emb, self.input)\n",
    "        x_emb = tf.reshape(x_emb,[-1,window_size*emb_size])\n",
    "        \n",
    "        y_one_hot = tf.one_hot(self.output,dictionary_size)\n",
    "\n",
    "        # Fully connetected layer\n",
    "        H = tf.Variable(tf.truncated_normal([window_size*emb_size, hidden_layer_size], stddev=0.1), name=\"H\")\n",
    "        d = tf.Variable(tf.constant(0.1, shape=[hidden_layer_size]), name=\"d\")\n",
    "        h1 = tf.tanh(tf.nn.xw_plus_b(x_emb, H, d))\n",
    "\n",
    "        #Regression layer\n",
    "        U = tf.Variable(tf.truncated_normal([hidden_layer_size, dictionary_size], stddev=0.1), name=\"U\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[dictionary_size]), name=\"b\")\n",
    "        W = tf.Variable(tf.truncated_normal([window_size*emb_size,dictionary_size],stddev=0.1), name=\"W\")\n",
    "\n",
    "        h2 = tf.add(tf.nn.xw_plus_b(h1, U, b),tf.matmul(x_emb,W))\n",
    "\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        l2_loss += tf.nn.l2_loss(H)\n",
    "        l2_loss += tf.nn.l2_loss(U)\n",
    "        l2_loss += tf.nn.l2_loss(W)\n",
    "        l2_loss += tf.nn.l2_loss(self.W_emb)\n",
    "\n",
    "\n",
    "        # prediction and loss function\n",
    "        self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=h2, labels=y_one_hot)\n",
    "        self.loss = tf.reduce_mean(self.losses) + l2_reg_lambda*l2_loss\n",
    "\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        self.sess = tf.Session(config=session_conf)  \n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(self, in_batch, out_batch):\n",
    "        feed_dict = {\n",
    "                self.input: in_batch,\n",
    "                self.output: out_batch\n",
    "                        }\n",
    "        _,loss, W_e = self.sess.run([self.optimizer, self.loss, self.W_emb], feed_dict)\n",
    "            # print (\"step \"+str(step) + \" loss \"+str(loss))\n",
    "        return W_e,loss\n",
    "\n",
    "\n",
    "dictiona = dict((word, i) for word, (i, _) in vocab_dict.items())\n",
    "reverse_dictionary = dict((i, word) for word, (i, _) in vocab_dict.items())\n",
    "data = [vocab_dict[word][0] for word in tokens_with_UNK]\n",
    "#count = pickle.load(handle)\n",
    "\n",
    "\n",
    "nplm = part1NPLM(len(dictionary))\n",
    "num_epochs = 1\n",
    "num_train = len(data) - 4\n",
    "batch_size = 512\n",
    "num_batches = num_train//batch_size\n",
    "window = 5\n",
    "\n",
    "W = []\n",
    "\n",
    "for j in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for k in range(batch_size):\n",
    "            curr_list = []\n",
    "            for p in range(window):\n",
    "                curr_list.append(data[i*batch_size+k+p])\n",
    "        \n",
    "            x_batch.append(curr_list)\t\n",
    "            y_batch.append(data[i*batch_size+k+window])\n",
    "\n",
    "        W, loss = nplm.train_step(x_batch,y_batch)\n",
    "        print(\"Epoch\",j+1,\"Batch\",i+1,\"Loss\",loss)\n",
    "        \n",
    "pickle.dump(W,open(\"nplm_emb.pickle\",\"wb\"))\n",
    "pickle.dump(vocab_dict,open(\"nlpm_dict.pickle\",\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
